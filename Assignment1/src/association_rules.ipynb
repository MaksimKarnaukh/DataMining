{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:24.957239700Z",
     "start_time": "2024-03-23T22:56:24.930313900Z"
    }
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "from helper_functions import load_dataset\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the cleaned dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54808ae25d6dd97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data: pd.DataFrame = None\n",
    "transactions: List[List] = None\n",
    "try:\n",
    "    data = load_dataset('../data/assignment1_income_levels_cleaned.xlsx')\n",
    "    transactions = data.astype(str).values.tolist()\n",
    "except FileNotFoundError:\n",
    "    print('File not found')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:27.256810700Z",
     "start_time": "2024-03-23T22:56:24.935298400Z"
    }
   },
   "id": "5e4cd67cef06d5b3",
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Search for Association Rules "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b4c76d716be41a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to run the apriori algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3119a6dc08041707"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_apriori(folder: str, transactions: List[List], min_support: float, min_confidence: float, min_length: int=2, sorted_by_support: bool=True, consequence:List[str]=None):\n",
    "    \"\"\"\n",
    "    Runs the apriori algorithm and writes the results (rules) to a txt file.\n",
    "    :param folder: folder where the output file should be saved\n",
    "    :param transactions: dataset as a list of lists\n",
    "    :param min_support: minimum support of relations\n",
    "    :param min_confidence: minimum confidence of relations\n",
    "    :param min_length: minimum number of items in a rule\n",
    "    :param sorted_by_support: True if the apriori results should be sorted by support, False otherwise\n",
    "    :param consequence: list of strings, where the rules should have at least one of the strings as their consequence\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    file_name: str = f'{folder}output_{min_support}_{min_confidence}_{min_length}.txt'\n",
    "    results: List[Tuple] = list(apriori(transactions, min_support=min_support, min_confidence=min_confidence, min_length=min_length))\n",
    "    nr_of_results: int = len(results) # this is the number of results returned by the apriori algorithm\n",
    "    nr_of_real_results: int = 0 # this is the number of results that have minimum length = 2\n",
    "    nr_of_rules: int = 0 # each result can have multiple rules, so here we count the total amount of association rules\n",
    "    row_list = []\n",
    "    try:\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(f\"Number of results: {nr_of_results}\\n\")\n",
    "            if sorted_by_support:\n",
    "                results = sorted(results, key=lambda x: x.support, reverse=True) # this means our final output will be sorted by support too\n",
    "            for rule in results:\n",
    "                if len(rule.items) == 1:\n",
    "                    continue\n",
    "                nr_of_real_results += 1\n",
    "                to_write: str = f\"Items: ({', '.join(rule.items)})\\n\" # for writing to txt file\n",
    "                is_consequence_present: bool = False\n",
    "                for i in range(0, len(rule.ordered_statistics)):\n",
    "                    antecedent: str = ', '.join(rule.ordered_statistics[i].items_base) # \"left side\" of the rule\n",
    "                    consequent: str = ', '.join(rule.ordered_statistics[i].items_add) # \"right side\" of the rule\n",
    "                    \n",
    "                    # skip rules with empty antecedent or consequent\n",
    "                    if len(antecedent) == 0 or len(consequent) == 0: \n",
    "                        continue\n",
    "                        \n",
    "                    # if we are looking for a specific consequence, we skip the rules that do not have it\n",
    "                    if consequence is not None and consequent not in consequence:\n",
    "                        continue\n",
    "                    is_consequence_present = True\n",
    "                    row_list.append({'antecedents': antecedent, 'consequents': consequent, 'support': rule.support, 'confidence': rule.ordered_statistics[i].confidence, 'lift': rule.ordered_statistics[i].lift})    \n",
    "                    support: str = str(rule.support)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "                    confidence: str = str(rule.ordered_statistics[i].confidence)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "                    lift: str = str(rule.ordered_statistics[i].lift)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "                \n",
    "                    to_write += f\"\\t{antecedent} => {consequent} (support: {support}, confidence: {confidence}, lift: {lift})\\n\"\n",
    "                    nr_of_rules += 1\n",
    "                    \n",
    "                to_write += '\\n'\n",
    "                if consequence is not None and not is_consequence_present:\n",
    "                    to_write = ''\n",
    "                file.write(to_write)\n",
    "            file.write(f\"Number of real results: {nr_of_real_results}\\n\") # this is the number of results that have minimum length = 2\n",
    "            file.write(f\"Number of total rules: {nr_of_rules}\\n\\n\")\n",
    "            \n",
    "        return pd.DataFrame(row_list, columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])                   \n",
    "    except FileNotFoundError:\n",
    "        print('File not found')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:27.274622700Z",
     "start_time": "2024-03-23T22:56:27.257808100Z"
    }
   },
   "id": "89fa2f7e9fb729bc",
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to generate a heatmap"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277c1776cf95ab93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_heatmap(folder: str, rules: pd.DataFrame, heatmap_value: str='support', min_support: float=0.1, min_confidence: float=0.8):\n",
    "    \"\"\"\n",
    "    Generates a heatmap from the rules dataframe, which are the resulting rules of the apriori algorithm.\n",
    "    The dataframe should have the following columns in this order: antecedents, consequents, support, confidence, lift.\n",
    "    Slightly based on https://medium.com/analytics-vidhya/market-basket-analysis-association-rule-mining-with-visualizations-cda24d537019\n",
    "    :param folder: folder where the heatmap should be saved\n",
    "    :param rules: dataframe with the rules\n",
    "    :param heatmap_value: the value to base on in the heatmap\n",
    "    :param min_support: minimum support that was used to generate the rules\n",
    "    :param min_confidence: minimum confidence that was used to generate the rules\n",
    "    \"\"\"\n",
    "    # Pivot the dataframe for heatmap\n",
    "    heatmap_data = rules.pivot(index='antecedents', columns='consequents', values=heatmap_value)\n",
    "    \n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Association Rules {heatmap_value} Heatmap for min_support={min_support} and min_confidence={min_confidence} ')\n",
    "    plt.xlabel('Consequents')\n",
    "    plt.ylabel('Antecedents')\n",
    "    plt.savefig(f'{folder}heatmap_{min_support}_{min_confidence}_{heatmap_value}.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:27.292816100Z",
     "start_time": "2024-03-23T22:56:27.277615200Z"
    }
   },
   "id": "d081e8715dc7a24d",
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Running the apriori algorithm\n",
    "\n",
    "Here, we play around with the algorithm and run it for different values for the \"minimum support\" and \"minimum confidence\" values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e41af2b6eef6d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "min_length: int = 2\n",
    "sorted_by_support: bool = True # if True, the results will be sorted by support\n",
    "\n",
    "supp_conf_values: List[Tuple] = [(0.2, 0.95), (0.6, 0.95), (0.8, 0.9), (0.8, 0.2), (0.6, 0.5), (0.2, 0.2)]\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules = run_apriori(\"../output/a/\", transactions, min_support, min_confidence, min_length, sorted_by_support)\n",
    "    if 1 <= len(rules) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/a/\", rules, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/a/\", rules, 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:36.753922800Z",
     "start_time": "2024-03-23T22:56:27.285834800Z"
    }
   },
   "id": "c388ab2c6da98afd",
   "execution_count": 90
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Running the apriori algorithm\n",
    "\n",
    "Here, we extract rules that have “sex = Male” or “sex = Female” as their consequence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8d8897d509c10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "supp_conf_values = [(0.1, 0.8), (0.6, 0.8), (0.9, 0.95), (0.8, 0.1)]\n",
    "consequence: List[str] = [\"Male\", \"Female\"]\n",
    "\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules = run_apriori(\"../output/b/\", transactions, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    if 1 <= len(rules) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/\", rules, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/\", rules, 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:45.811553300Z",
     "start_time": "2024-03-23T22:56:36.754919900Z"
    }
   },
   "id": "73e4a7b8d8e0ddec",
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "# splitting the data dataset into two separate datasets, one with only \"Male\" sex and another with only \"Female\" sex\n",
    "data_male = data[data['sex'] == \"Male\"]\n",
    "data_female = data[data['sex'] == \"Female\"]\n",
    "\n",
    "transactions_male = data_male.astype(str).values.tolist()\n",
    "transactions_female = data_female.astype(str).values.tolist()\n",
    "\n",
    "supp_conf_values = [(0.2, 0.6)]\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules_male = run_apriori(\"../output/b/split/male_\", transactions_male, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    rules_female = run_apriori(\"../output/b/split/female_\", transactions_female, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    if 1 <= len(rules_male) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/male_\", rules_male, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/male_\", rules_male, 'confidence', min_support, min_confidence)\n",
    "    if 1 <= len(rules_female) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/female_\", rules_female, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/female_\", rules_female, 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T22:56:53.215639700Z",
     "start_time": "2024-03-23T22:56:45.814547300Z"
    }
   },
   "id": "9adacb4ab7b20d8b",
   "execution_count": 92
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
