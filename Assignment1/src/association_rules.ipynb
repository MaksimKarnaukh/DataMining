{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:32.856525200Z",
     "start_time": "2024-03-29T00:27:32.845350900Z"
    }
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "from helper_functions import load_dataset\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the cleaned dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54808ae25d6dd97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data: pd.DataFrame = None\n",
    "transactions: List[List] = None\n",
    "try:\n",
    "    data = load_dataset('../data/assignment2_income_levels_cleaned.xlsx')\n",
    "    transactions = data.astype(str).values.tolist()\n",
    "except FileNotFoundError:\n",
    "    print('File not found')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:34.882690700Z",
     "start_time": "2024-03-29T00:27:32.851528800Z"
    }
   },
   "id": "5e4cd67cef06d5b3",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Search for Association Rules "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3b4c76d716be41a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to run the apriori algorithm"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3119a6dc08041707"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def run_apriori(folder: str, transactions: List[List], min_support: float, min_confidence: float, min_length: int=2, sorted_by_support: bool=True, consequence:List[str]=None):\n",
    "    \"\"\"\n",
    "    Runs the apriori algorithm and writes the results (rules) to a txt file.\n",
    "    :param folder: folder where the output file should be saved. If None, the file will not be saved\n",
    "    :param transactions: dataset as a list of lists\n",
    "    :param min_support: minimum support of relations\n",
    "    :param min_confidence: minimum confidence of relations\n",
    "    :param min_length: minimum number of items in a rule\n",
    "    :param sorted_by_support: True if the apriori results should be sorted by support, False otherwise\n",
    "    :param consequence: list of strings, so that the rules should have at least one of the strings as their consequence\n",
    "    :return: dataframe with the rules\n",
    "    \"\"\"\n",
    "    file_name: str = f'{folder}output_{min_support}_{min_confidence}_{min_length}.txt'\n",
    "    results: List[Tuple] = list(apriori(transactions, min_support=min_support, min_confidence=min_confidence, min_length=min_length))\n",
    "    nr_of_results: int = len(results) # this is the number of results returned by the apriori algorithm\n",
    "    nr_of_real_results: int = 0 # this is the number of results that have minimum length = 2\n",
    "    nr_of_rules: int = 0 # each result can have multiple rules, so here we count the total amount of association rules\n",
    "    row_list = [] # list of dictionaries, where each dictionary represents a row in the (to be generated) dataframe\n",
    "    file_txt = \"\"\n",
    "    try:\n",
    "        if sorted_by_support:\n",
    "            results = sorted(results, key=lambda x: x.support, reverse=True) # this means our final output will be sorted by support too\n",
    "        for rule in results:\n",
    "            if len(rule.items) == 1:\n",
    "                continue\n",
    "            nr_of_real_results += 1\n",
    "            to_write: str = f\"Items: ({', '.join(rule.items)})\\n\" # for writing to txt file\n",
    "            is_consequence_present: bool = False\n",
    "            for i in range(0, len(rule.ordered_statistics)):\n",
    "                antecedent: str = ', '.join(rule.ordered_statistics[i].items_base) # \"left side\" of the rule\n",
    "                consequent: str = ', '.join(rule.ordered_statistics[i].items_add) # \"right side\" of the rule\n",
    "                \n",
    "                # skip rules with empty antecedent or consequent\n",
    "                if len(antecedent) == 0 or len(consequent) == 0: \n",
    "                    continue\n",
    "                    \n",
    "                # if we are looking for a specific consequence, we skip the rules that do not have it\n",
    "                if consequence is not None:\n",
    "                    skip = True\n",
    "                    for c in consequence:\n",
    "                        if c in consequent:\n",
    "                            skip = False\n",
    "                            break\n",
    "                    if skip:\n",
    "                        continue\n",
    "                is_consequence_present = True\n",
    "                \n",
    "                row_list.append({'antecedents': antecedent, 'consequents': consequent, 'support': rule.support, 'confidence': rule.ordered_statistics[i].confidence, 'lift': rule.ordered_statistics[i].lift})    \n",
    "                \n",
    "                support: str = str(rule.support)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "                confidence: str = str(rule.ordered_statistics[i].confidence)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "                lift: str = str(rule.ordered_statistics[i].lift)[:7] # float, so we need to convert it to string and limit the number of decimal places\n",
    "            \n",
    "                to_write += f\"\\t{antecedent} => {consequent} (support: {support}, confidence: {confidence}, lift: {lift})\\n\"\n",
    "                nr_of_rules += 1\n",
    "                \n",
    "            to_write += '\\n'\n",
    "            if consequence is not None and not is_consequence_present:\n",
    "                to_write = ''\n",
    "            file_txt += to_write\n",
    "            \n",
    "        if folder is not None:\n",
    "            with open(file_name, 'w') as file:\n",
    "                file.write(f\"Number of results: {nr_of_results}\\n\")\n",
    "                file.write(f\"Number of real results: {nr_of_real_results}\\n\") # this is the number of results that have minimum length = 2\n",
    "                file.write(f\"Number of total rules: {nr_of_rules}\\n\\n\")\n",
    "                file.write(file_txt)\n",
    "            \n",
    "        return pd.DataFrame(row_list, columns=['antecedents', 'consequents', 'support', 'confidence', 'lift'])                   \n",
    "    except FileNotFoundError:\n",
    "        print('File not found')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:34.904079900Z",
     "start_time": "2024-03-29T00:27:34.884679Z"
    }
   },
   "id": "89fa2f7e9fb729bc",
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Function to generate a heatmap"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "277c1776cf95ab93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_heatmap(folder: str, rules: pd.DataFrame, heatmap_value: str='support', min_support: float=0.1, min_confidence: float=0.8):\n",
    "    \"\"\"\n",
    "    Generates a heatmap from the rules dataframe, which are the resulting rules of the apriori algorithm.\n",
    "    The dataframe should have the following columns in this order: antecedents, consequents, support, confidence, lift.\n",
    "    Slightly based on https://medium.com/analytics-vidhya/market-basket-analysis-association-rule-mining-with-visualizations-cda24d537019\n",
    "    :param folder: folder where the heatmap should be saved\n",
    "    :param rules: dataframe with the rules\n",
    "    :param heatmap_value: the value to base on in the heatmap\n",
    "    :param min_support: minimum support that was used to generate the rules\n",
    "    :param min_confidence: minimum confidence that was used to generate the rules\n",
    "    \"\"\"\n",
    "    # Pivot the dataframe for heatmap\n",
    "    heatmap_data = rules.pivot(index='antecedents', columns='consequents', values=heatmap_value)\n",
    "    \n",
    "    # Generate the heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "    plt.title(f'Association Rules {heatmap_value} Heatmap for min_support={min_support} and min_confidence={min_confidence} ')\n",
    "    plt.xlabel('Consequents')\n",
    "    plt.ylabel('Antecedents')\n",
    "    plt.savefig(f'{folder}heatmap_{min_support}_{min_confidence}_{heatmap_value}.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:34.917078Z",
     "start_time": "2024-03-29T00:27:34.901087400Z"
    }
   },
   "id": "d081e8715dc7a24d",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Running the apriori algorithm\n",
    "\n",
    "Here, we play around with the algorithm and run it for different values for the \"minimum support\" and \"minimum confidence\" values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88e41af2b6eef6d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "min_length: int = 2\n",
    "sorted_by_support: bool = True # if True, the results will be sorted by support\n",
    "\n",
    "supp_conf_values: List[Tuple] = [(0.2, 0.95), (0.6, 0.95), (0.8, 0.9), (0.8, 0.2), (0.6, 0.5), (0.2, 0.2)]\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules: pd.DataFrame = run_apriori(\"../output/a/\", transactions, min_support, min_confidence, min_length, sorted_by_support)\n",
    "    if 1 <= len(rules) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/a/\", rules, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/a/\", rules, 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:43.757293500Z",
     "start_time": "2024-03-29T00:27:34.912092900Z"
    }
   },
   "id": "c388ab2c6da98afd",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we investigate the influence of minimum support and confidence on the number of association rules. We run the apriori algorithm for different values of minimum support with fixed minimum confidence (0.05) and for different values of minimum confidence with a fixed minimum support (0.05), and plot the number of rules for each case."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ca6cad2114edc7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "to_plot = False # set to True if you want to generate the plot below. Beware, it will take some time to generate the plot (10-15 minutes)\n",
    "if to_plot:\n",
    "    starting_sup_val: float = 0.05 # fixed minimum support value\n",
    "    starting_conf_val: float = 0.05 # fixed minimum confidence value\n",
    "    \n",
    "    sup_val: float = starting_sup_val\n",
    "    conf_val: float = starting_conf_val\n",
    "    \n",
    "    sup_rules: List[Tuple] = [] # list of tuples, where each tuple contains the minimum support value and the resulting number of rules\n",
    "    conf_rules: List[Tuple] = [] # list of tuples, where each tuple contains the minimum confidence value and the resulting number of rules\n",
    "    \n",
    "    while sup_val < 1:\n",
    "        rules: pd.DataFrame = run_apriori(None, transactions, sup_val, starting_conf_val, min_length, sorted_by_support)\n",
    "        sup_rules.append((sup_val, len(rules)))\n",
    "        sup_val += 0.05 # increase the minimum support value by 0.05, so we will have around 20 iterations\n",
    "        rules = run_apriori(None, transactions, starting_sup_val, conf_val, min_length, sorted_by_support)\n",
    "        conf_rules.append((conf_val, len(rules)))\n",
    "        conf_val += 0.05 # same here\n",
    "    \n",
    "    x_sup, y_sup = zip(*sup_rules)\n",
    "    x_conf, y_conf = zip(*conf_rules)\n",
    "    \n",
    "    # Plot the data\n",
    "    plt.plot(x_sup, y_sup, label='Minimum support')\n",
    "    plt.plot(x_conf, y_conf, label='Minimum confidence')\n",
    "    plt.xlabel('Minimum support and confidence')\n",
    "    plt.ylabel('Number of rules')\n",
    "    plt.title('Influence of minimum support and confidence on number of association rules')\n",
    "    plt.grid(True)\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:27:43.769487500Z",
     "start_time": "2024-03-29T00:27:43.763968800Z"
    }
   },
   "id": "52cfbac7fe858f8e",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Running the apriori algorithm\n",
    "\n",
    "Here, we extract rules that have “sex = Male” or “sex = Female” as their consequence. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8d8897d509c10"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "supp_conf_values = [(0.05, 0.2), (0.1, 0.8)] # above minimum support= 0.4, there are no rules\n",
    "consequence: List[str] = [\"Male\", \"Female\"]\n",
    "\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules: pd.DataFrame = run_apriori(\"../output/b/\", transactions, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    rules_males: pd.DataFrame = rules[(rules['consequents'] == 'Male') & \n",
    "                             (~rules['antecedents'].astype(str).str.contains('Husband')) & \n",
    "                             (~rules['antecedents'].astype(str).str.contains('No')) & \n",
    "                             (~rules['antecedents'].astype(str).str.contains('0'))] # only male as consequent + some extra filtering\n",
    "    rules_female: pd.DataFrame = rules[(rules['consequents'] == 'Female') &\n",
    "                             (~rules['antecedents'].astype(str).str.contains('Wife')) & \n",
    "                             (~rules['antecedents'].astype(str).str.contains('No')) & \n",
    "                             (~rules['antecedents'].astype(str).str.contains('0'))]# only female as consequent + some extra filtering\n",
    "    if 1 <= len(rules_males):\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/male_\", rules_males.head(25), 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/male_\", rules_males.head(25), 'confidence', min_support, min_confidence)\n",
    "    if 1 <= len(rules_female):\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/female_\", rules_female.head(25), 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/female_\", rules_female.head(25), 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:28:30.791954200Z",
     "start_time": "2024-03-29T00:27:43.769487500Z"
    }
   },
   "id": "73e4a7b8d8e0ddec",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we do the same but for the cases where the dataset is split into two separate datasets (Male vs Female). In hindsight, this is not so useful."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "517b1890d053fb91"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%capture\n",
    "# splitting the data dataset into two separate datasets, one with only \"Male\" sex and another with only \"Female\" sex\n",
    "data_male: pd.DataFrame = data[data['sex'] == \"Male\"]\n",
    "data_female: pd.DataFrame = data[data['sex'] == \"Female\"]\n",
    "\n",
    "transactions_male = data_male.astype(str).values.tolist()\n",
    "transactions_female = data_female.astype(str).values.tolist()\n",
    "\n",
    "supp_conf_values = [(0.2, 0.6)]\n",
    "for min_support, min_confidence in supp_conf_values:\n",
    "    rules_male: pd.DataFrame = run_apriori(\"../output/b/split/male_\", transactions_male, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    rules_female: pd.DataFrame = run_apriori(\"../output/b/split/female_\", transactions_female, min_support, min_confidence, min_length, sorted_by_support, consequence) \n",
    "    if 1 <= len(rules_male) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/male_\", rules_male, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/male_\", rules_male, 'confidence', min_support, min_confidence)\n",
    "    if 1 <= len(rules_female) < 250:\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/female_\", rules_female, 'support', min_support, min_confidence)\n",
    "        generate_heatmap(\"../plots/assoc_rules/b/split/female_\", rules_female, 'confidence', min_support, min_confidence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-29T00:28:33.984302600Z",
     "start_time": "2024-03-29T00:28:30.792951700Z"
    }
   },
   "id": "9adacb4ab7b20d8b",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
