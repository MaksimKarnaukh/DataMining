{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:48.172896Z",
     "start_time": "2024-04-13T01:35:48.060145700Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from helper.helper_functions import load_dataset, save_model, get_features_and_target, encode_all_features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#:~:text=This%20Sequential%20Feature%20Selector%20adds,validation%20score%20of%20an%20estimator.\n",
    "def seq_feat_selection(model, X_train: pd.DataFrame, y_train: pd.Series, direction: str = \"backwards\", scoring: str = \"accuracy\"):\n",
    "    \n",
    "    # Create Sequential Feature Selector\n",
    "    sfs = SequentialFeatureSelector(model, \n",
    "                                     direction=direction, # backward feature elimination\n",
    "                                     scoring=scoring,\n",
    "                                     cv=StratifiedKFold())\n",
    "    \n",
    "    # Fit the feature selector to training data\n",
    "    sfs.fit(X_train, y_train)\n",
    "    \n",
    "    # Get selected features and feature indices\n",
    "    selected_features = sfs.get_support()\n",
    "    selected_feature_indices = [i for i, val in enumerate(selected_features) if val]\n",
    "    \n",
    "    print(\"Selected features:\")\n",
    "    for i, feature in enumerate(X_train.columns[selected_feature_indices]):\n",
    "        print(f\"Feature {i+1}: {feature}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:48.196832200Z",
     "start_time": "2024-04-13T01:35:48.174890900Z"
    }
   },
   "id": "dae8d8ede3964cf4",
   "execution_count": 134
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# https://medium.com/@agrawalsam1997/hyperparameter-tuning-of-knn-classifier-a32f31af25c7\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "def tune_knn(model, X_train: pd.DataFrame, y_train: pd.Series, X_test, y_test, param_grid) -> Tuple[dict, KNeighborsClassifier, float]:\n",
    "    \"\"\"\n",
    "    Tune the hyperparameters of the K-Nearest Neighbors model using GridSearchCV.\n",
    "    :param X_encoded: \n",
    "    :param y_encoded: \n",
    "    :param X_test: \n",
    "    :param y_test: \n",
    "    :return: \n",
    "    \"\"\"    \n",
    "    # Perform grid search with 5-fold cross-validation\n",
    "    gscv = GridSearchCV(model, param_grid=param_grid, cv=5, verbose=1)\n",
    "    \n",
    "    gscv.fit(X_train, y_train)\n",
    "    \n",
    "    best_params = gscv.best_params_ \n",
    "    best_model = gscv.best_estimator_ \n",
    "    best_accuracy = best_model.score(X_test, y_test)\n",
    "    \n",
    "    return best_params, best_model, best_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:48.225552600Z",
     "start_time": "2024-04-13T01:35:48.199824500Z"
    }
   },
   "id": "7a6425fa2762c28f",
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def forward_feat_selection_hypertuning(X_encoded: pd.DataFrame, y_encoded: pd.Series) -> Tuple[List[str], dict, float]:\n",
    "    \"\"\"\n",
    "    Forward feature selection with hyperparameter tuning for K-Nearest Neighbors.\n",
    "    :param X_encoded: features dataset, with features encoded\n",
    "    :param y_encoded: target dataset, with target encoded\n",
    "    :return: (best subset of features, best hyperparameters, best model accuracy)\n",
    "    \"\"\"\n",
    "    best_subset: List[str] = []\n",
    "    best_params: dict = None\n",
    "    best_score: float = 0.0\n",
    "    \n",
    "    remaining_features = [['age'], \n",
    "                          ['education'], \n",
    "                          ['workinghours'],\n",
    "                          [col for col in X_encoded.columns if col.startswith('workclass')],\n",
    "                          [col for col in X_encoded.columns if col.startswith('marital status')],\n",
    "                          [col for col in X_encoded.columns if col.startswith('occupation')]]\n",
    "    \n",
    "    param_grid = {\n",
    "                'n_neighbors': np.arange(2, 30, 1),\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2]\n",
    "    }\n",
    "    \n",
    "    while remaining_features:\n",
    "        subset_scores = []\n",
    "        subset_params = []\n",
    "    \n",
    "        for feature_cat in remaining_features:\n",
    "            # Combine the current best subset with the new feature\n",
    "            current_subset = best_subset + feature_cat if best_subset else feature_cat.copy()\n",
    "            \n",
    "            X_subset = X_encoded[current_subset]\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_subset, y_encoded, test_size=0.2, random_state=42)\n",
    "            \n",
    "            best_params, best_model, score = tune_knn(KNeighborsClassifier(), X_subset, y_encoded, X_val, y_val, param_grid)\n",
    "            \n",
    "            subset_scores.append(score)\n",
    "            subset_params.append(best_params)\n",
    "            \n",
    "        # Select the feature that improves performance the most\n",
    "        best_index = subset_scores.index(max(subset_scores))\n",
    "        best_score = subset_scores[best_index]\n",
    "        best_params = subset_params[best_index]\n",
    "        best_feature = remaining_features[best_index]\n",
    "        \n",
    "        # Update the best subset and remaining features\n",
    "        best_subset = best_subset + best_feature if best_subset else best_feature.copy()\n",
    "        del remaining_features[best_index]\n",
    "        \n",
    "        print(\"Best subset:\", best_subset)\n",
    "        print(\"Remaining features:\", remaining_features)\n",
    "        \n",
    "    return best_subset, best_params, best_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:48.238517300Z",
     "start_time": "2024-04-13T01:35:48.214581300Z"
    }
   },
   "id": "f63480a3daebdb62",
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Both functions below are heavily based on the following example from the scikit-learn documentation:\n",
    "https://scikit-learn.org/stable/auto_examples/model_selection/plot_multi_metric_evaluation.html#sphx-glr-auto-examples-model-selection-plot-multi-metric-evaluation-py\n",
    "\"\"\"\n",
    "\n",
    "def multi_metric_cv(model, scoring, X_train, y_train, param_grid, refit=\"AUC\"):\n",
    "    \"\"\"\n",
    "    Perform GridSearchCV with multiple scorers for a given model.\n",
    "    :param model: \n",
    "    :param scoring: \n",
    "    :param X_train: \n",
    "    :param y_train: \n",
    "    :param param_grid: \n",
    "    :param refit: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Initialize GridSearchCV\n",
    "    gs = GridSearchCV(\n",
    "        model,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        refit=refit,\n",
    "        n_jobs=2,\n",
    "        return_train_score=True,\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    gs.fit(X_train, y_train)\n",
    "    results = gs.cv_results_\n",
    "    \n",
    "    return results\n",
    "    \n",
    "def plot_multi_score_cv_results(x_label, x_min_val, x_max_val, y_min_val, y_max_val, results, param_name, param_type, scoring):\n",
    "    \"\"\"\n",
    "    Plot the results of GridSearchCV with multiple scorers.\n",
    "    :param x_label: \n",
    "    :param x_min_val: \n",
    "    :param x_max_val: \n",
    "    :param y_min_val: \n",
    "    :param y_max_val: \n",
    "    :param results: \n",
    "    :param param_name: \n",
    "    :param param_type: \n",
    "    :param scoring: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(13, 13))\n",
    "    plt.title(\"GridSearchCV evaluating using multiple scorers simultaneously\", fontsize=16)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim(x_min_val, x_max_val-1)\n",
    "    ax.set_ylim(y_min_val, y_max_val)\n",
    "    \n",
    "    # Get the regular numpy array from the MaskedArray\n",
    "    X_axis = np.array(results[param_name].data, dtype=param_type)\n",
    "    colors = [\"g\", \"b\", \"r\", \"y\", \"m\", \"k\"]\n",
    "    \n",
    "    for scorer, color in zip(sorted(scoring), colors[:len(scoring)]):\n",
    "        for sample, style in ((\"train\", \"--\"), (\"test\", \"-\")):\n",
    "            sample_score_mean = results[\"mean_%s_%s\" % (sample, scorer)]\n",
    "            sample_score_std = results[\"std_%s_%s\" % (sample, scorer)]\n",
    "            ax.fill_between(\n",
    "                X_axis,\n",
    "                sample_score_mean - sample_score_std,\n",
    "                sample_score_mean + sample_score_std,\n",
    "                alpha=0.1 if sample == \"test\" else 0,\n",
    "                color=color,\n",
    "            )\n",
    "            ax.plot(\n",
    "                X_axis,\n",
    "                sample_score_mean,\n",
    "                style,\n",
    "                color=color,\n",
    "                alpha=1 if sample == \"test\" else 0.7,\n",
    "                label=\"%s (%s)\" % (scorer, sample),\n",
    "            )\n",
    "    \n",
    "        best_index = np.nonzero(results[\"rank_test_%s\" % scorer] == 1)[0][0]\n",
    "        best_score = results[\"mean_test_%s\" % scorer][best_index]\n",
    "    \n",
    "        # Plot a dotted vertical line at the best score for that scorer marked by x\n",
    "        ax.plot(\n",
    "            [\n",
    "                X_axis[best_index],\n",
    "            ]\n",
    "            * 2,\n",
    "            [0, best_score],\n",
    "            linestyle=\"-.\",\n",
    "            color=color,\n",
    "            marker=\"x\",\n",
    "            markeredgewidth=3,\n",
    "            ms=8,\n",
    "        )\n",
    "    \n",
    "        # Annotate the best score for that scorer\n",
    "        ax.annotate(\"%0.2f\" % best_score, (X_axis[best_index], best_score + 0.005))\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(False)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:48.267443700Z",
     "start_time": "2024-04-13T01:35:48.236522900Z"
    }
   },
   "id": "b8a31ff35fe61b73",
   "execution_count": 137
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the cleaned dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ced45577f2a46d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_dataset('../data/assignment2_income_cleaned.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:50.855211700Z",
     "start_time": "2024-04-13T01:35:48.247497500Z"
    }
   },
   "id": "573b15a0c08becd2",
   "execution_count": 138
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target (y)\n",
    "X, y = get_features_and_target(data, 'income')\n",
    "columns_to_exclude = ['ability to speak english', 'gave birth this year']\n",
    "# Encoding the features and target, and excluding some columns\n",
    "X_encoded, y_encoded = encode_all_features(X, y, columns_to_exclude)\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:50.892495100Z",
     "start_time": "2024-04-13T01:35:50.856209400Z"
    }
   },
   "id": "83fc7449a1cf23ea",
   "execution_count": 139
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc5f29026270e84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.80      0.80      1175\n",
      "           1       0.62      0.62      0.62       625\n",
      "\n",
      "    accuracy                           0.74      1800\n",
      "   macro avg       0.71      0.71      0.71      1800\n",
      "weighted avg       0.74      0.74      0.74      1800\n",
      "\n",
      "K-Nearest Neighbors Accuracy: 0.7388888888888889\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
    "knn_model.fit(X_train, y_train)\n",
    "# Predictions\n",
    "knn_preds = knn_model.predict(X_test)\n",
    "# Accuracy evaluation\n",
    "knn_accuracy = accuracy_score(y_test, knn_preds)\n",
    "\n",
    "print(classification_report(y_test, knn_preds))\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.045147700Z",
     "start_time": "2024-04-13T01:35:50.891486700Z"
    }
   },
   "id": "7e89004d3b62eb17",
   "execution_count": 140
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "seq_feat_sel = False\n",
    "if seq_feat_sel:\n",
    "    seq_feat_selection(KNeighborsClassifier(), X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.054119800Z",
     "start_time": "2024-04-13T01:35:51.043332500Z"
    }
   },
   "id": "c0c47f5934a16da",
   "execution_count": 141
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1951416f12714f18"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tune = False\n",
    "if tune:\n",
    "    param_grid = {\n",
    "                'n_neighbors': np.arange(2, 30, 1),\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'p': [1, 2]\n",
    "    }\n",
    "    \n",
    "    best_params, best_model, best_accuracy = tune_knn(KNeighborsClassifier(), X_train, y_train, X_test, y_test, param_grid)\n",
    "    \n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "    print(\"Best Model:\", best_model)\n",
    "    print(\"Best Model Accuracy:\", best_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.062568Z",
     "start_time": "2024-04-13T01:35:51.049131800Z"
    }
   },
   "id": "c7365ea88fe88b3a",
   "execution_count": 142
  },
  {
   "cell_type": "markdown",
   "source": [
    "Best Hyperparameters: {'n_neighbors': 28, 'p': 1, 'weights': 'uniform'}, Best Model Accuracy: 0.7777777777777778"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "854252d0794f8351"
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://www.geeksforgeeks.org/feature-selection-techniques-in-machine-learning/\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83291e7b2a927e64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "multi_metric_cv = False\n",
    "if multi_metric_cv:\n",
    "    # Define the scorers\n",
    "    scoring = {\"AUC\": \"roc_auc\", \"Accuracy\": make_scorer(accuracy_score)}\n",
    "    \n",
    "    n_neighbours_param_min = 2\n",
    "    n_neighbours_param_max = 31\n",
    "    param_grid = {\"n_neighbors\": range(n_neighbours_param_min, n_neighbours_param_max, 1)}\n",
    "    \n",
    "    results = multi_metric_cv(KNeighborsClassifier(), scoring, X_train, y_train, param_grid)\n",
    "    plot_multi_score_cv_results(\"n_neighbors\", n_neighbours_param_min, n_neighbours_param_max, 0.7, 1, results, \"param_n_neighbors\", int, scoring)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.102020800Z",
     "start_time": "2024-04-13T01:35:51.061570100Z"
    }
   },
   "id": "ecb1487c222d91",
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Forward feature selection with hyperparameter tuning\n",
    "if False:\n",
    "    best_subset, best_params, best_score = forward_feat_selection_hypertuning(X_train, y_train)\n",
    "    \n",
    "    print(\"Best subset of features:\", best_subset)\n",
    "    print(\"Best hyperparameters:\", best_params)\n",
    "    print(\"Best model accuracy:\", best_score)\n",
    "    \n",
    "    # Use the best subset and best hyperparameters for final model\n",
    "    final_model = KNeighborsClassifier(**best_params)\n",
    "    final_model.fit(X_train[best_subset], y_train)\n",
    "    final_model_preds = final_model.predict(X_test[best_subset])\n",
    "    final_model_accuracy = accuracy_score(y_test, final_model_preds)\n",
    "    \n",
    "    print(classification_report(y_test, final_model_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.104015500Z",
     "start_time": "2024-04-13T01:35:51.072703500Z"
    }
   },
   "id": "ef0a2ac8b804d296",
   "execution_count": 144
  },
  {
   "cell_type": "markdown",
   "source": [
    "Feature Selection: Ensure that sensitive attributes such as sex are not included as features in the model. This helps prevent the model from directly learning discriminatory patterns based on sensitive attributes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca92285d6580a173"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fair Representation: Balance the representation of different groups within the dataset. Ensure that the dataset is representative of the population with respect to sex and other sensitive attributes. This can help mitigate bias in the model's predictions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "629aa7c9a8929a30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fair Preprocessing: Apply preprocessing techniques that aim to mitigate bias in the dataset. For example, use techniques such as reweighting or resampling to balance the dataset with respect to sensitive attributes.\n",
    "\n",
    "Reweighing:\n",
    "Adjust the weights of instances in your dataset to balance the impact of different groups.\n",
    "Assign higher weights to underrepresented groups (e.g., females) and lower weights to overrepresented groups (e.g., males).\n",
    "This helps reduce bias during model training.\n",
    "Suppression:\n",
    "Remove or suppress the sensitive attribute (‘sex’) from the dataset.\n",
    "Train your model without using this attribute.\n",
    "Note that this approach may lead to information loss, so use it cautiously.\n",
    "Massaging the Dataset:\n",
    "Modify class labels to achieve fairness.\n",
    "For instance, you can swap the labels for different groups (e.g., relabel some ‘male’ instances as ‘female’ and vice versa).\n",
    "This ensures that the model does not learn discriminatory patterns based on the sensitive attribute.\n",
    "Resampling Techniques:\n",
    "Oversample the underrepresented group (e.g., females) or undersample the overrepresented group (e.g., males).\n",
    "This balances the class distribution and reduces bias.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1f7bc109c2e2782"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Evaluation: Evaluate the model's performance and fairness across different subgroups defined by sensitive attributes such as sex. Use metrics such as disparate impact ratio, equal opportunity difference, or predictive parity to assess fairness."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a61e1afff765d7f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disparate Impact (DI): 0.49\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_test['sex'] = X_test['sex_Male'] * 1\n",
    "X_train['sex'] = X_train['sex_Male'] * 1\n",
    "X_test = X_test.drop(columns=['sex_Male', 'sex_Female'])\n",
    "X_train = X_train.drop(columns=['sex_Male', 'sex_Female'])\n",
    "\n",
    "# Train your KNN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predicted labels from the KNN model\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "# calculate DI:\n",
    "# DI = (num_positives(priviliged=False) / num_instances(priviliged=False)) / (num_positives(priviliged=True) / num_instances(priviliged=True))\n",
    "num_instances_priv_false = X_test[X_test['sex']==0].shape[0]\n",
    "num_instances_priv_true = X_test[X_test['sex']==1].shape[0]\n",
    "\n",
    "# Calculate the number of positive outcomes and instances for each group\n",
    "num_positives_priv_false = np.sum((y_pred == 1) & (X_test['sex'] == 0))\n",
    "num_positives_priv_true = np.sum((y_pred == 1) & (X_test['sex'] == 1))\n",
    "\n",
    "# Calculate the Disparate Impact (DI) criterion\n",
    "DI = (num_positives_priv_false / num_instances_priv_false) / (num_positives_priv_true / num_instances_priv_true)\n",
    "\n",
    "print(f\"Disparate Impact (DI): {DI:.2f}\")\n",
    "\n",
    "# # Get predicted labels for females and males\n",
    "# y_pred_female = y_pred[X_test['sex'] == 0]\n",
    "# y_pred_male = y_pred[X_test['sex'] == 1]\n",
    "# \n",
    "# # Calculate the probability of predicted positive outcomes for females and males\n",
    "# prob_positive_female = np.mean(y_pred_female)\n",
    "# prob_positive_male = np.mean(y_pred_male)\n",
    "# \n",
    "# # Calculate the Disparate Impact (DI) criterion\n",
    "# DI = prob_positive_female / prob_positive_male\n",
    "# \n",
    "# print(f\"Disparate Impact (DI): {DI:.2f}\")\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = knn_model.predict(X_test)\n",
    "# \n",
    "# # Create a confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# \n",
    "# # Extract counts for each group (male and female)\n",
    "# male_count = np.sum(X_test['sex_Male'])\n",
    "# female_count = np.sum(X_test['sex_Female'])\n",
    "# \n",
    "# # Calculate favorable outcomes (e.g., 'income' = 1) for each group\n",
    "# male_favorable = conf_matrix[1, 1]\n",
    "# female_favorable = conf_matrix[0, 1]\n",
    "# \n",
    "# # Calculate disparate impact\n",
    "# disparate_impact = (female_favorable / female_count) / (male_favorable / male_count)\n",
    "# \n",
    "# # Print the result\n",
    "# print(f\"Disparate Impact: {disparate_impact:.2f}\")\n",
    "\n",
    "# Interpretation:\n",
    "# - If disparate impact = 1, outcomes are equally favorable for both groups.\n",
    "# - If disparate impact > 1, one group has more favorable outcomes than the other.\n",
    "# - If disparate impact < 1, the opposite is true.\n",
    "\n",
    "# You can set a threshold (e.g., 0.8) to determine fairness based on your context."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.309739800Z",
     "start_time": "2024-04-13T01:35:51.085066200Z"
    }
   },
   "id": "f4c859f9827b2601",
   "execution_count": 145
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "array([[940, 235],\n       [235, 390]], dtype=int64)"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix # Thus in binary classification, the count of true negatives is C0,0, false negatives is C1,0, true positives is C1,1 and false positives is C0,1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.319224Z",
     "start_time": "2024-04-13T01:35:51.306748200Z"
    }
   },
   "id": "4b389689201399af",
   "execution_count": 146
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['sex_Male', 'sex_Female'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[147], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Disparate Impact Ratio\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01maif360\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m disparate_impact_ratio\n\u001B[1;32m----> 3\u001B[0m disparate_impact \u001B[38;5;241m=\u001B[39m disparate_impact_ratio(y_true\u001B[38;5;241m=\u001B[39my_test, y_pred\u001B[38;5;241m=\u001B[39my_pred, prot_attr\u001B[38;5;241m=\u001B[39m\u001B[43mX_test\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msex_Male\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msex_Female\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mto_numpy())\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDisparate Impact Ratio:\u001B[39m\u001B[38;5;124m\"\u001B[39m, disparate_impact)\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4096\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4094\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   4095\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 4096\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   4098\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   4099\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nmissing:\n\u001B[0;32m   6248\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m nmissing \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(indexer):\n\u001B[1;32m-> 6249\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6251\u001B[0m     not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m   6252\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"None of [Index(['sex_Male', 'sex_Female'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Disparate Impact Ratio\n",
    "from aif360.sklearn.metrics import disparate_impact_ratio\n",
    "disparate_impact = disparate_impact_ratio(y_true=y_test, y_pred=y_pred, prot_attr=X_test[['sex_Male', 'sex_Female']].to_numpy())\n",
    "print(\"Disparate Impact Ratio:\", disparate_impact)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T01:35:51.393274200Z",
     "start_time": "2024-04-13T01:35:51.318227300Z"
    }
   },
   "id": "50504ced1d517660",
   "execution_count": 147
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# from aif360.sklearn.metrics import statistical_parity_difference\n",
    "# \n",
    "# # encode the 'sex' feature\n",
    "# X['sex'] = X['sex'].map({'Male': 0, 'Female': 1})\n",
    "# y = y.map({'low': 0, 'high': 1})\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# \n",
    "# # Train your KNN model\n",
    "# knn_model = KNeighborsClassifier()\n",
    "# knn_model.fit(X_train, y_train)\n",
    "# \n",
    "# # Predictions\n",
    "# knn_preds = knn_model.predict(X_test)\n",
    "# \n",
    "# # Calculate fairness metrics using model predictions and specify 'sex' as the sensitive attribute\n",
    "# spd_sex = statistical_parity_difference(y_true=y_test, y_pred=knn_preds, sensitive_features=X_test['sex'])\n",
    "# eo = equal_opportunity_difference(y_true=y_test, y_pred=knn_preds, sensitive_features=X_test['sex'])\n",
    "# aod = average_odds_difference(y_true=y_test, y_pred=knn_preds, sensitive_features=X_test['sex'])\n",
    "# apv = average_predictive_value_difference(y_true=y_test, y_pred=knn_preds, sensitive_features=X_test['sex'])\n",
    "# \n",
    "# print(\"Statistical Parity Difference:\", spd_sex)\n",
    "# print(\"Equal Opportunity Difference:\", eo)\n",
    "# print(\"Average Odds Difference:\", aod)\n",
    "# print(\"Average Predictive Value Difference:\", apv)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T01:35:51.389284800Z"
    }
   },
   "id": "48c8cb536e714e01",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "save_model(final_model, '../output/saved_models/knn_model.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-13T01:35:51.391279900Z"
    }
   },
   "id": "6a153de59eafb6a6",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
