{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:04.162494600Z",
     "start_time": "2024-04-25T02:30:03.938228300Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "from helper.helper_functions import load_dataset, save_model, get_features_and_target, encode_all_features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the cleaned dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ced45577f2a46d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_dataset('../data/assignment2_income_cleaned.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.242500700Z",
     "start_time": "2024-04-25T02:30:04.162494600Z"
    }
   },
   "id": "573b15a0c08becd2",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering (encoding) & Train-Test Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7592687113789b2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target (y)\n",
    "X, y = get_features_and_target(data, 'income')\n",
    "# Encoding the features and target, and excluding some columns\n",
    "X_encoded, y_encoded = encode_all_features(X, y, [])\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.270851400Z",
     "start_time": "2024-04-25T02:30:06.244495800Z"
    }
   },
   "id": "6aef9cfa1cf6ddd3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "X_train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "495ea34f27ff398f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Here, we quickly train and evaluate a Gaussian Naive Bayes model for demonstration."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc5f29026270e84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.73      0.78      1175\n",
      "           1       0.58      0.71      0.64       625\n",
      "\n",
      "    accuracy                           0.72      1800\n",
      "   macro avg       0.70      0.72      0.71      1800\n",
      "weighted avg       0.74      0.72      0.73      1800\n",
      "\n",
      "Naive Bayes Accuracy: 0.7227777777777777\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes model (Gaussian)\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, y_train)\n",
    "nb_preds = nb_model.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
    "\n",
    "print(classification_report(y_test, nb_preds))\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.303308500Z",
     "start_time": "2024-04-25T02:30:06.269854200Z"
    }
   },
   "id": "7c4fcfd7603fcff1",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined Naive Bayes Model: Custom Implementation\n",
    "\n",
    "Below we implement and train a Combined Naive Bayes model for mixed data types (categorical and numerical)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "595fb7cd421a19ff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.naive_bayes import GaussianNB, CategoricalNB\n",
    "\n",
    "class CombinedNB:\n",
    "    \"\"\"\n",
    "    Combined Naive Bayes model for mixed data types (categorical and numerical)\n",
    "    This looks a bit like a sklearn classifier class, but it's not (it doesn't inherit from anything). It's just a simple class that implements the fit and predict methods only, for two combined Naive Bayes models.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_feat: List[str], cat_model: CategoricalNB = CategoricalNB(), num_model: GaussianNB = GaussianNB()):\n",
    "        self.cat_model: CategoricalNB = cat_model\n",
    "        self.num_model: GaussianNB = num_model\n",
    "        self.num_feat: List[str] = num_feat # numerical features\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the combined model to the training data. We fit the categorical model to the categorical features and the numerical model to the numerical features.\n",
    "        :param X_train: training dataset with mixed features\n",
    "        :param y_train: training dataset with target variable\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        # Splitting data into categorical and numerical features\n",
    "        X_train_cat = X_train.drop(columns=self.num_feat)\n",
    "        X_train_num = X_train[self.num_feat]\n",
    "\n",
    "        self.cat_model.fit(X_train_cat, y_train)\n",
    "        self.num_model.fit(X_train_num, y_train)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the target variable for the test data.\n",
    "        \n",
    "        We use the formula: \n",
    "        P(Class | C1, ..., Cn, N1, ..., Nm) ~= Mult{1, n}(P(Ci | Class)) * Mult{1, m}(P(Nj | Class)) * P(Class)\n",
    "        \n",
    "        where Ci are the categorical features, Nj are the numerical features, and Class is the target variable. \n",
    "        Concerning the scores, we get the following:\n",
    "        Cat * Num * Prior = (Score_cat * Score_num) / Prior\n",
    "\n",
    "        Note: CategoricalNB uses Laplace smoothing by default.\n",
    "        \n",
    "        :param X_test: test dataset with mixed features\n",
    "        :return: predicted target variable\n",
    "        \"\"\"\n",
    "        cat_probs = self.cat_model.predict_proba(X_test.drop(columns=self.num_feat))\n",
    "        num_probs = self.num_model.predict_proba(X_test[self.num_feat])\n",
    "        combined_probs = (cat_probs * num_probs) / self.num_model.class_prior_\n",
    "        # combined_probs /= combined_probs.sum(axis=1, keepdims=True) # normalize the probabilities\n",
    "        return np.argmax(combined_probs, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.313824500Z",
     "start_time": "2024-04-25T02:30:06.301313600Z"
    }
   },
   "id": "33c11ade77e2c784",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing the Combined Naive Bayes Model\n",
    "\n",
    "Below is a small test function for the Combined Naive Bayes model. We will test the model on a small dataset to check if it's generally working correctly. The test is based on the example from the lecture slides by Toon Calders."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfe2fdcca4589a97"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_combined_nb_model(print_mapping=False):\n",
    "    data_ = {\n",
    "    'Refund': ['Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No'],\n",
    "    'Marital Status': ['Single', 'Married', 'Single', 'Married', 'Divorced', 'Married', 'Divorced', 'Single', 'Married', 'Single', 'Divorced'],\n",
    "    'Taxable Income': [125, 100, 70, 120, 95, 60, 220, 85, 75, 90, 120],\n",
    "    'Evade': ['No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No']\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data_)\n",
    "    \n",
    "    # Encoding features and target\n",
    "    le_ = LabelEncoder()\n",
    "    for col in df.columns:\n",
    "        if col not in ['Taxable Income']:\n",
    "            enc = le_.fit_transform(df[col])\n",
    "            df[col] = enc\n",
    "            # print encoding mapping\n",
    "            if print_mapping:\n",
    "                print(f\"Mapping for {col}:\")\n",
    "                for original, encoded in zip(le_.classes_, range(len(le_.classes_))):\n",
    "                    print(f\"{original} -> {encoded}\")\n",
    "    \n",
    "    # Separate features (X) and target variable (y)\n",
    "    X_ = df.iloc[:, :-1]\n",
    "    y_ = df.iloc[:, -1]\n",
    "    \n",
    "    # Splitting data into training and test sets\n",
    "    X_train_ = X_.iloc[:-1]\n",
    "    X_test_ = X_.iloc[-1:]\n",
    "    y_train_ = y_.iloc[:-1]\n",
    "    y_test_ = y_.iloc[-1:]\n",
    "    \n",
    "    comb_nb = CombinedNB(num_feat=['Taxable Income'])\n",
    "    comb_nb.fit(X_train_, y_train_)\n",
    "    comb_nb_preds = comb_nb.predict(X_test_)\n",
    "        \n",
    "    assert (comb_nb.cat_model.category_count_[0][0] == [4., 3.]).all()\n",
    "    assert (comb_nb.cat_model.category_count_[0][1] == [3., 0.]).all()\n",
    "    assert (comb_nb.cat_model.category_count_[1][0] == [1., 4., 2.]).all()\n",
    "    assert (comb_nb.cat_model.category_count_[1][1] == [1., 0., 2.]).all()\n",
    "    assert (comb_nb.num_model.class_count_ == np.array([7., 3.])).all()\n",
    "    assert (comb_nb.num_model.class_prior_ == np.array([0.7, 0.3])).all()\n",
    "    assert (comb_nb.num_model.theta_ == np.array([[110.], [ 90.]])).all()\n",
    "    assert (np.round(comb_nb.num_model.var_) == np.round(np.array([[2550.00000187], [16.66666854]]))).all()\n",
    "    assert comb_nb_preds == 0\n",
    "    \n",
    "    # convert log probabilities to probabilities\n",
    "    feature_probs_0 = np.exp(comb_nb.cat_model.feature_log_prob_[0])\n",
    "    feature_probs_1 = np.exp(comb_nb.cat_model.feature_log_prob_[1])\n",
    "    \n",
    "    assert (np.round(feature_probs_0[0], 2) == np.round(np.array([0.55555556, 0.44444444]), 2)).all()\n",
    "    assert (np.round(feature_probs_0[1], 2) == np.round(np.array([0.8, 0.2]), 2)).all()\n",
    "    assert (np.round(feature_probs_1[0], 2) == np.round(np.array([0.2, 0.5, 0.3]), 2)).all()\n",
    "    assert (np.round(feature_probs_1[1], 2) == np.round(np.array([0.33333333, 0.16666667, 0.5]), 2)).all()\n",
    "    \n",
    "    print(\"All tests passed successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.360581400Z",
     "start_time": "2024-04-25T02:30:06.313824500Z"
    }
   },
   "id": "74872c98f1e6a1da",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_combined_nb_model()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.365567200Z",
     "start_time": "2024-04-25T02:30:06.331148200Z"
    }
   },
   "id": "76a52f0b30900338",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training and Evaluating the Combined Naive Bayes Model\n",
    "\n",
    "Since Naive Bayes uses the independence assumption, one-hot encoding features is not a smart idea. We will use label encoding for all categorical features. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "477cae3f83c449"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Naive Bayes Accuracy: 0.7738888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82      1175\n",
      "           1       0.66      0.72      0.69       625\n",
      "\n",
      "    accuracy                           0.77      1800\n",
      "   macro avg       0.75      0.76      0.76      1800\n",
      "weighted avg       0.78      0.77      0.78      1800\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X.copy()\n",
    "# encode all categorical features using label encoding\n",
    "le = LabelEncoder()\n",
    "for col in X_encoded.columns:\n",
    "    if col not in ['education', 'ability to speak english', 'age', 'workinghours']:\n",
    "        X_encoded[col] = le.fit_transform(X[col])\n",
    "                \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "comb_nb = CombinedNB(num_feat=['age', 'workinghours'])\n",
    "comb_nb.fit(X_train, y_train)\n",
    "comb_nb_preds = comb_nb.predict(X_test)\n",
    "comb_nb_accuracy = accuracy_score(y_test, comb_nb_preds)\n",
    "\n",
    "print(\"Combined Naive Bayes Accuracy:\", comb_nb_accuracy)\n",
    "print(classification_report(y_test, comb_nb_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.414406800Z",
     "start_time": "2024-04-25T02:30:06.358587400Z"
    }
   },
   "id": "5341fe8a2cb42db9",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mixed Naive Bayes Model: library implementation\n",
    "\n",
    "I only later found out this library implementation of Mixed Naive Bayes, although I don't know its internal wokrings. We can use it too to compare the results."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83d4cf95bba046c3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Naive Bayes Accuracy: 0.7755555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.81      0.82      1175\n",
      "           1       0.66      0.72      0.69       625\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.75      0.76      0.76      1800\n",
      "weighted avg       0.78      0.78      0.78      1800\n"
     ]
    }
   ],
   "source": [
    "from mixed_naive_bayes import MixedNB # https://pypi.org/project/mixed-naive-bayes/#api-documentation\n",
    "\n",
    "# the library implementation of MixedNB requires the categorical features to be encoded as integers starting from 0, so we need to encode the education feature separately.\n",
    "le = LabelEncoder()\n",
    "X_train['education'] = le.fit_transform(X_train['education'])\n",
    "X_test['education'] = le.transform(X_test['education'])\n",
    "\n",
    "comb_nb = MixedNB(categorical_features=[1,2,3,4,6,7,8]) # indices of categorical features\n",
    "comb_nb.fit(X_train, y_train)\n",
    "comb_nb_preds = comb_nb.predict(X_test)\n",
    "comb_nb_accuracy = accuracy_score(y_test, comb_nb_preds)\n",
    "\n",
    "print(\"Mixed Naive Bayes Accuracy:\", comb_nb_accuracy)\n",
    "print(classification_report(y_test, comb_nb_preds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.448015600Z",
     "start_time": "2024-04-25T02:30:06.407426100Z"
    }
   },
   "id": "7db6160745e4247c",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Categorical Naive Bayes Model\n",
    "\n",
    "Finally, we can try categorizing the numerical features and training a Categorical Naive Bayes model on a dataset that now has only categorical features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3866426f79e38661"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83      1175\n",
      "           1       0.67      0.74      0.70       625\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.76      0.77      0.76      1800\n",
      "weighted avg       0.79      0.78      0.78      1800\n",
      "\n",
      "Naive Bayes Accuracy: 0.7811111111111111\n"
     ]
    }
   ],
   "source": [
    "X_encoded = X.copy()\n",
    "# we categorize the age and workinghours features, so that all features are categorical\n",
    "X_encoded = X_encoded.drop(columns=['age', 'workinghours'])\n",
    "X_encoded['age'] = pd.cut(X['age'], bins=[0,28,38,49,65,93], labels=['(17-28]', '(28-38]', '(38-49]', '(49-65]', '(65-93]'])\n",
    "X_encoded['workinghours'] = pd.cut(X['workinghours'], bins=[0, 30, 40, 99], labels=['Part-time', 'Full-time', 'Overtime'])\n",
    "\n",
    "# encode all categorical features using label encoding\n",
    "le = LabelEncoder()\n",
    "for col in X_encoded.columns:\n",
    "    if col not in ['education', 'ability to speak english']:\n",
    "        X_encoded[col] = le.fit_transform(X[col])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "nb_cat = CategoricalNB()\n",
    "nb_cat.fit(X_train, y_train)\n",
    "nb_preds = nb_cat.predict(X_test)\n",
    "nb_accuracy = accuracy_score(y_test, nb_preds)\n",
    "\n",
    "print(classification_report(y_test, nb_preds))\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.505205600Z",
     "start_time": "2024-04-25T02:30:06.444027Z"
    }
   },
   "id": "e2578ad3831e4c15",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Naive Bayes models do not have many hyperparameters to tune. We won't do any hyperparameter tuning for any models here."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9423e3ffd4469d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the Model(s)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ce32c604f62593"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# save model\n",
    "save_model(nb_model, '../output/saved_models/naive_bayes_model.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T02:30:06.516785500Z",
     "start_time": "2024-04-25T02:30:06.503210900Z"
    }
   },
   "id": "eac15450e6ee09ea",
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
