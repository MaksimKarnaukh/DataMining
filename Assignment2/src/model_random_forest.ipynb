{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:33.379022800Z",
     "start_time": "2024-05-04T21:33:27.016978Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'tensorflow': AdversarialDebiasing will be unavailable. To install, run:\n",
      "pip install 'aif360[AdversarialDebiasing]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'inFairness': SenSeI and SenSR will be unavailable. To install, run:\n",
      "pip install 'aif360[inFairness]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'ot': ot_distance will be unavailable. To install, run:\n",
      "pip install 'aif360[OptimalTransport]'\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from helper.helper_functions import load_dataset, save_model, get_features_and_target, encode_all_features, get_train_test_with_excluded_columns\n",
    "from helper.clfmodel_functions import tune_model, forward_feat_selection_hypertuning\n",
    "from helper.fairness_functions import print_male_female_metrics, get_male_female_data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading the cleaned dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ced45577f2a46d2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = load_dataset('../data/assignment2_income_cleaned.xlsx')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:37.722502500Z",
     "start_time": "2024-05-04T21:33:33.373039700Z"
    }
   },
   "id": "573b15a0c08becd2",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Engineering (encoding) & Train-Test Split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d2f4fd682304c4c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Splitting the data into features (X) and target (y)\n",
    "X, y = get_features_and_target(data, 'income')\n",
    "X_male, X_female = get_male_female_data(X, False)\n",
    "# Encoding the features and target, and excluding some columns\n",
    "X_encoded, y_encoded = encode_all_features(X, y, [])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:37.915812500Z",
     "start_time": "2024-05-04T21:33:37.724497900Z"
    }
   },
   "id": "6aef9cfa1cf6ddd3",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "      age  education  workinghours  ability to speak english  \\\n6317   22         16            36                         0   \n740    61         22            40                         1   \n3781   48         16            40                         0   \n7850   62         18            65                         0   \n2963   53         19            44                         0   \n...   ...        ...           ...                       ...   \n5734   22         19            25                         0   \n5191   24         16            28                         0   \n5390   35         16            40                         0   \n860    23         20            40                         0   \n7270   70         19            40                         0   \n\n      marital status_Divorced  marital status_Husband  \\\n6317                        0                       1   \n740                         1                       0   \n3781                        1                       0   \n7850                        0                       1   \n2963                        1                       0   \n...                       ...                     ...   \n5734                        0                       0   \n5191                        0                       0   \n5390                        0                       0   \n860                         0                       0   \n7270                        1                       0   \n\n      marital status_Never married  marital status_Separated  \\\n6317                             0                         0   \n740                              0                         0   \n3781                             0                         0   \n7850                             0                         0   \n2963                             0                         0   \n...                            ...                       ...   \n5734                             1                         0   \n5191                             1                         0   \n5390                             1                         0   \n860                              1                         0   \n7270                             0                         0   \n\n      marital status_Widowed  marital status_Wife  ...  \\\n6317                       0                    0  ...   \n740                        0                    0  ...   \n3781                       0                    0  ...   \n7850                       0                    0  ...   \n2963                       0                    0  ...   \n...                      ...                  ...  ...   \n5734                       0                    0  ...   \n5191                       0                    0  ...   \n5390                       0                    0  ...   \n860                        0                    0  ...   \n7270                       0                    0  ...   \n\n      occupation_Service/Hospitality  occupation_Transport  \\\n6317                               0                     0   \n740                                0                     0   \n3781                               0                     0   \n7850                               0                     0   \n2963                               0                     0   \n...                              ...                   ...   \n5734                               1                     0   \n5191                               0                     0   \n5390                               0                     0   \n860                                1                     0   \n7270                               0                     0   \n\n      gave birth this year_No  gave birth this year_Yes  sex_Female  sex_Male  \\\n6317                        1                         0           0         1   \n740                         1                         0           0         1   \n3781                        1                         0           1         0   \n7850                        1                         0           0         1   \n2963                        1                         0           1         0   \n...                       ...                       ...         ...       ...   \n5734                        1                         0           1         0   \n5191                        1                         0           1         0   \n5390                        1                         0           0         1   \n860                         1                         0           0         1   \n7270                        1                         0           1         0   \n\n      workclass_governmental  workclass_no paid work  workclass_private  \\\n6317                       0                       0                  1   \n740                        0                       0                  0   \n3781                       0                       0                  1   \n7850                       0                       0                  1   \n2963                       1                       0                  0   \n...                      ...                     ...                ...   \n5734                       0                       0                  1   \n5191                       0                       0                  1   \n5390                       0                       0                  1   \n860                        0                       0                  1   \n7270                       0                       1                  0   \n\n      workclass_self employed  \n6317                        0  \n740                         1  \n3781                        0  \n7850                        0  \n2963                        0  \n...                       ...  \n5734                        0  \n5191                        0  \n5390                        0  \n860                         0  \n7270                        0  \n\n[7200 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>education</th>\n      <th>workinghours</th>\n      <th>ability to speak english</th>\n      <th>marital status_Divorced</th>\n      <th>marital status_Husband</th>\n      <th>marital status_Never married</th>\n      <th>marital status_Separated</th>\n      <th>marital status_Widowed</th>\n      <th>marital status_Wife</th>\n      <th>...</th>\n      <th>occupation_Service/Hospitality</th>\n      <th>occupation_Transport</th>\n      <th>gave birth this year_No</th>\n      <th>gave birth this year_Yes</th>\n      <th>sex_Female</th>\n      <th>sex_Male</th>\n      <th>workclass_governmental</th>\n      <th>workclass_no paid work</th>\n      <th>workclass_private</th>\n      <th>workclass_self employed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6317</th>\n      <td>22</td>\n      <td>16</td>\n      <td>36</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>61</td>\n      <td>22</td>\n      <td>40</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3781</th>\n      <td>48</td>\n      <td>16</td>\n      <td>40</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7850</th>\n      <td>62</td>\n      <td>18</td>\n      <td>65</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2963</th>\n      <td>53</td>\n      <td>19</td>\n      <td>44</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5734</th>\n      <td>22</td>\n      <td>19</td>\n      <td>25</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5191</th>\n      <td>24</td>\n      <td>16</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5390</th>\n      <td>35</td>\n      <td>16</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>860</th>\n      <td>23</td>\n      <td>20</td>\n      <td>40</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7270</th>\n      <td>70</td>\n      <td>19</td>\n      <td>40</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>7200 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:38.033328500Z",
     "start_time": "2024-05-04T21:33:37.917808300Z"
    }
   },
   "id": "220e372cd3ad9944",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "Here, we quickly train and evaluate a Random Forest model with random parameters for demonstration."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc5f29026270e84"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82      1175\n",
      "           1       0.66      0.64      0.65       625\n",
      "\n",
      "    accuracy                           0.76      1800\n",
      "   macro avg       0.74      0.73      0.73      1800\n",
      "weighted avg       0.76      0.76      0.76      1800\n",
      "\n",
      "Decision Tree Accuracy: 0.7622222222222222\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "dt_preds = rf_model.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
    "\n",
    "print(classification_report(y_test, dt_preds))\n",
    "print(\"Decision Tree Accuracy:\", dt_accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:39.355332100Z",
     "start_time": "2024-05-04T21:33:38.014378100Z"
    }
   },
   "id": "755b22d9e1afea95",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Importance using the model itself"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80c8c2ebf3fe25a8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:39.400713700Z",
     "start_time": "2024-05-04T21:33:39.341369800Z"
    }
   },
   "id": "1caff6200c3a0805",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                         Feature  Importance\n0                                            age    0.308890\n1                                      education    0.173548\n2                                   workinghours    0.153756\n5                         marital status_Husband    0.050358\n6                   marital status_Never married    0.033286\n18                occupation_Management/Business    0.028194\n25   occupation_Science, Engineering, Technology    0.022273\n26                occupation_Service/Hospitality    0.020277\n34                             workclass_private    0.014773\n31                                      sex_Male    0.014666\n30                                    sex_Female    0.012957\n32                        workclass_governmental    0.012757\n3                       ability to speak english    0.011810\n35                       workclass_self employed    0.011187\n24                              occupation_Sales    0.010891\n21                occupation_Production/Assembly    0.010240\n16        occupation_Healthcare/Medical Services    0.010239\n20      occupation_Office/Administrative Support    0.010194\n10            occupation_Construction/Extraction    0.009537\n27                          occupation_Transport    0.009516\n4                        marital status_Divorced    0.009322\n23                 occupation_Repair/Maintenance    0.009143\n9                            marital status_Wife    0.008009\n12                          occupation_Education    0.007277\n15                 occupation_Finance/Accounting    0.006093\n22                occupation_Protective Services    0.005075\n11  occupation_Counseling/Mental Health Services    0.004739\n8                         marital status_Widowed    0.004368\n17                     occupation_Legal Services    0.003869\n7                       marital status_Separated    0.003413\n13                      occupation_Entertainment    0.002595\n14         occupation_Farming, Fishing, Forestry    0.002058\n19                  occupation_Military Services    0.001305\n29                      gave birth this year_Yes    0.001255\n28                       gave birth this year_No    0.001219\n33                        workclass_no paid work    0.000911",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Feature</th>\n      <th>Importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>age</td>\n      <td>0.308890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>education</td>\n      <td>0.173548</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>workinghours</td>\n      <td>0.153756</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>marital status_Husband</td>\n      <td>0.050358</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>marital status_Never married</td>\n      <td>0.033286</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>occupation_Management/Business</td>\n      <td>0.028194</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>occupation_Science, Engineering, Technology</td>\n      <td>0.022273</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>occupation_Service/Hospitality</td>\n      <td>0.020277</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>workclass_private</td>\n      <td>0.014773</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>sex_Male</td>\n      <td>0.014666</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>sex_Female</td>\n      <td>0.012957</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>workclass_governmental</td>\n      <td>0.012757</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ability to speak english</td>\n      <td>0.011810</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>workclass_self employed</td>\n      <td>0.011187</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>occupation_Sales</td>\n      <td>0.010891</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>occupation_Production/Assembly</td>\n      <td>0.010240</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>occupation_Healthcare/Medical Services</td>\n      <td>0.010239</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>occupation_Office/Administrative Support</td>\n      <td>0.010194</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>occupation_Construction/Extraction</td>\n      <td>0.009537</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>occupation_Transport</td>\n      <td>0.009516</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>marital status_Divorced</td>\n      <td>0.009322</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>occupation_Repair/Maintenance</td>\n      <td>0.009143</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>marital status_Wife</td>\n      <td>0.008009</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>occupation_Education</td>\n      <td>0.007277</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>occupation_Finance/Accounting</td>\n      <td>0.006093</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>occupation_Protective Services</td>\n      <td>0.005075</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>occupation_Counseling/Mental Health Services</td>\n      <td>0.004739</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>marital status_Widowed</td>\n      <td>0.004368</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>occupation_Legal Services</td>\n      <td>0.003869</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>marital status_Separated</td>\n      <td>0.003413</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>occupation_Entertainment</td>\n      <td>0.002595</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>occupation_Farming, Fishing, Forestry</td>\n      <td>0.002058</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>occupation_Military Services</td>\n      <td>0.001305</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>gave birth this year_Yes</td>\n      <td>0.001255</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>gave birth this year_No</td>\n      <td>0.001219</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>workclass_no paid work</td>\n      <td>0.000911</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:39.405701900Z",
     "start_time": "2024-05-04T21:33:39.369558400Z"
    }
   },
   "id": "a00036dc0784a239",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a2dc56e20a45256"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we define the base parameter grid for our hyperparameter tuning function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b741e43c3d433553"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "    'random_state': [42]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T21:33:39.426773Z",
     "start_time": "2024-05-04T21:33:39.394730800Z"
    }
   },
   "id": "c29e653b05e109c8",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we tune the hyperparameters of the Random Forest model using the defined parameter grid and using all features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f2cd85f21ea6108"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "\n",
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'log_loss', 'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 42}\n",
      "Best Model: RandomForestClassifier(criterion='log_loss', max_depth=20, min_samples_split=10,\n",
      "                       n_estimators=200, random_state=42)\n",
      "Best Model Accuracy: 0.7883333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      1175\n",
      "           1       0.71      0.67      0.69       625\n",
      "\n",
      "    accuracy                           0.79      1800\n",
      "   macro avg       0.77      0.76      0.76      1800\n",
      "weighted avg       0.79      0.79      0.79      1800\n",
      "Fairness Metrics:\n",
      "Male FPR: 0.19040902679830748\n",
      "Male TPR: 0.7334669338677354\n",
      "Female FPR: 0.0815450643776824\n",
      "Female TPR: 0.40476190476190477\n",
      "\n",
      "Disparate Impact (DI): 0.362\n",
      "Discrimination Score (DS): -0.264\n",
      "Equal Opportunity Difference (EO): 0.329\n",
      "Equalized Odds (EOdds): 0.109\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model, best_accuracy = tune_model(RandomForestClassifier(), X_encoded, X_train, y_train, X_test, y_test, param_grid)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "rf_preds = best_model.predict(X_test)\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test, y_test)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_unfair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T22:41:08.172039100Z",
     "start_time": "2024-05-04T21:33:39.412810700Z"
    }
   },
   "id": "b58b63df3b376fb3",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a937a7c03c78f756"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "\n",
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 42}\n",
      "Best Model: RandomForestClassifier(max_depth=30, min_samples_leaf=4, min_samples_split=10,\n",
      "                       n_estimators=200, random_state=42)\n",
      "Best Model Accuracy: 0.7855555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84      1175\n",
      "           1       0.71      0.64      0.67       625\n",
      "\n",
      "    accuracy                           0.79      1800\n",
      "   macro avg       0.77      0.75      0.76      1800\n",
      "weighted avg       0.78      0.79      0.78      1800\n",
      "Fairness Metrics:\n",
      "Male FPR: 0.18053596614950634\n",
      "Male TPR: 0.6933867735470942\n",
      "Female FPR: 0.07081545064377683\n",
      "Female TPR: 0.42857142857142855\n",
      "\n",
      "Disparate Impact (DI): 0.375\n",
      "Discrimination Score (DS): -0.245\n",
      "Equal Opportunity Difference (EO): 0.265\n",
      "Equalized Odds (EOdds): 0.110\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model, best_accuracy = tune_model(RandomForestClassifier(), X_encoded, X_train, y_train, X_test, y_test, param_grid, ensure_fairness=True)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "rf_preds = best_model.predict(X_test)\n",
    "print(classification_report(y_test, rf_preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test, y_test)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_fair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-04T23:46:50.944958200Z",
     "start_time": "2024-05-04T22:41:08.173035900Z"
    }
   },
   "id": "e9e5721c8f666037",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we tune the hyperparameters of the Random Forest model using the defined parameter grid and using a **subset of features**. We exclude the columns 'age', 'ability to speak english', and 'workclass'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39b916338a8ee5ad"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "\n",
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 500, 'random_state': 42}\n",
      "Best Model: RandomForestClassifier(max_depth=20, min_samples_leaf=4, min_samples_split=10,\n",
      "                       n_estimators=500, random_state=42)\n",
      "Best Model Accuracy: 0.7794444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.86      0.84      1175\n",
      "           1       0.71      0.62      0.66       625\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.76      0.74      0.75      1800\n",
      "weighted avg       0.77      0.78      0.78      1800\n",
      "Fairness Metrics:\n",
      "Male FPR: 0.18758815232722145\n",
      "Male TPR: 0.6773547094188377\n",
      "Female FPR: 0.055793991416309016\n",
      "Female TPR: 0.3888888888888889\n",
      "\n",
      "Disparate Impact (DI): 0.325\n",
      "Discrimination Score (DS): -0.263\n",
      "Equal Opportunity Difference (EO): 0.288\n",
      "Equalized Odds (EOdds): 0.132\n"
     ]
    }
   ],
   "source": [
    "columns_to_exclude = ['age', 'ability to speak english', 'workclass']\n",
    "X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "\n",
    "best_params, best_model, best_accuracy = tune_model(RandomForestClassifier(), X_encoded, X_train_, y_train_, X_test_, y_test_, param_grid)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "rf_preds = best_model.predict(X_test_)\n",
    "print(classification_report(y_test_, rf_preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test_, y_test_)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_unfair_aawexcl.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T02:02:23.413468500Z",
     "start_time": "2024-05-05T01:05:24.007998Z"
    }
   },
   "id": "a059b0e8a56a5377",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b646317774b98e4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "\n",
      "Best Hyperparameters: {'bootstrap': True, 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 42}\n",
      "Best Model: RandomForestClassifier(max_depth=20, min_samples_leaf=4, min_samples_split=10,\n",
      "                       random_state=42)\n",
      "Best Model Accuracy: 0.7794444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      1175\n",
      "           1       0.71      0.62      0.66       625\n",
      "\n",
      "    accuracy                           0.78      1800\n",
      "   macro avg       0.76      0.74      0.75      1800\n",
      "weighted avg       0.77      0.78      0.78      1800\n",
      "\n",
      "Fairness Metrics:\n",
      "Male FPR: 0.1847672778561354\n",
      "Male TPR: 0.6733466933867736\n",
      "Female FPR: 0.05793991416309013\n",
      "Female TPR: 0.3968253968253968\n",
      "\n",
      "Disparate Impact (DI): 0.336\n",
      "Discrimination Score (DS): -0.257\n",
      "Equal Opportunity Difference (EO): 0.277\n",
      "Equalized Odds (EOdds): 0.127\n"
     ]
    }
   ],
   "source": [
    "columns_to_exclude = ['age', 'ability to speak english', 'workclass']\n",
    "X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "\n",
    "best_params, best_model, best_accuracy = tune_model(RandomForestClassifier(), X_encoded, X_train_, y_train_, X_test_, y_test_, param_grid, ensure_fairness=True)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "rf_preds = best_model.predict(X_test_)\n",
    "print(classification_report(y_test_, rf_preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test_, y_test_)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_fair_aawexcl.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T02:53:54.213743200Z",
     "start_time": "2024-05-05T02:02:23.418441600Z"
    }
   },
   "id": "8172907135a9f68",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Forward Feature Selection with Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebecaf1b73c5f9aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because hyperparameter tuning results can vary depending on the chosen subset of features, we will use forward feature selection with hyperparameter tuning to find the best subset of features and hyperparameters for the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe647a90e359a21a"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n",
      "Fitting 5 folds for each of 486 candidates, totalling 2430 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Forward feature selection with hyperparameter tuning\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m best_subset, best_params, best_score \u001B[38;5;241m=\u001B[39m \u001B[43mforward_feat_selection_hypertuning\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRandomForestClassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest subset of features:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_subset)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBest hyperparameters:\u001B[39m\u001B[38;5;124m\"\u001B[39m, best_params)\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\Assignment2\\src\\helper\\clfmodel_functions.py:179\u001B[0m, in \u001B[0;36mforward_feat_selection_hypertuning\u001B[1;34m(model, param_grid, X_encoded, X_train, y_train, X_val, y_val, epsilon, ensure_fairness, columns_to_exclude)\u001B[0m\n\u001B[0;32m    176\u001B[0m X_subset: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m X_train[current_subset]\n\u001B[0;32m    177\u001B[0m X_val_subset: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m X_val[current_subset]\n\u001B[1;32m--> 179\u001B[0m best_params, best_model, score \u001B[38;5;241m=\u001B[39m \u001B[43mtune_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_val_subset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m                                            \u001B[49m\u001B[43mensure_fairness\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mensure_fairness\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m subset_scores\u001B[38;5;241m.\u001B[39mappend(score)\n\u001B[0;32m    183\u001B[0m subset_params\u001B[38;5;241m.\u001B[39mappend(best_params)\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\Assignment2\\src\\helper\\clfmodel_functions.py:63\u001B[0m, in \u001B[0;36mtune_model\u001B[1;34m(model, X_encoded, X_train, y_train, X_test, y_test, param_grid, top_n, cv, ensure_fairness)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;66;03m# Perform grid search with 5-fold cross-validation\u001B[39;00m\n\u001B[0;32m     62\u001B[0m gscv: GridSearchCV \u001B[38;5;241m=\u001B[39m GridSearchCV(model, param_grid\u001B[38;5;241m=\u001B[39mparam_grid, cv\u001B[38;5;241m=\u001B[39mcv, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 63\u001B[0m \u001B[43mgscv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     65\u001B[0m top_n_results_idx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margsort(gscv\u001B[38;5;241m.\u001B[39mcv_results_[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmean_test_score\u001B[39m\u001B[38;5;124m'\u001B[39m])[\u001B[38;5;241m-\u001B[39mtop_n:]\n\u001B[0;32m     67\u001B[0m best_params: \u001B[38;5;28mdict\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, **params)\u001B[0m\n\u001B[0;32m    964\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    965\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    966\u001B[0m     )\n\u001B[0;32m    968\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 970\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    974\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001B[0m, in \u001B[0;36mGridSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1525\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1526\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1527\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\u001B[43mParameterGrid\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    908\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    910\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    911\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    912\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    913\u001B[0m         )\n\u001B[0;32m    914\u001B[0m     )\n\u001B[1;32m--> 916\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    920\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    922\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    924\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplitter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    935\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    936\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    937\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    938\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    939\u001B[0m     )\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1863\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1861\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[0;32m   1862\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1863\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(output)\n\u001B[0;32m   1865\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[0;32m   1868\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[0;32m   1870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\joblib\\parallel.py:1792\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1790\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1791\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m-> 1792\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[1;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001B[0m, in \u001B[0;36m_fit_and_score\u001B[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001B[0m\n\u001B[0;32m    893\u001B[0m         estimator\u001B[38;5;241m.\u001B[39mfit(X_train, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\n\u001B[0;32m    894\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 895\u001B[0m         \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m    898\u001B[0m     \u001B[38;5;66;03m# Note fit time as time until error\u001B[39;00m\n\u001B[0;32m    899\u001B[0m     fit_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start_time\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1472\u001B[0m     )\n\u001B[0;32m   1473\u001B[0m ):\n\u001B[1;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:478\u001B[0m, in \u001B[0;36mBaseForest.fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarm_start \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# We draw from the random state to get the random state we\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;66;03m# would have got if we hadn't used a warm_start.\u001B[39;00m\n\u001B[0;32m    476\u001B[0m     random_state\u001B[38;5;241m.\u001B[39mrandint(MAX_INT, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_))\n\u001B[1;32m--> 478\u001B[0m trees \u001B[38;5;241m=\u001B[39m \u001B[43m[\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_estimator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mn_more_estimators\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[0;32m    489\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    490\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    491\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    508\u001B[0m )\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:479\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwarm_start \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    474\u001B[0m     \u001B[38;5;66;03m# We draw from the random state to get the random state we\u001B[39;00m\n\u001B[0;32m    475\u001B[0m     \u001B[38;5;66;03m# would have got if we hadn't used a warm_start.\u001B[39;00m\n\u001B[0;32m    476\u001B[0m     random_state\u001B[38;5;241m.\u001B[39mrandint(MAX_INT, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_))\n\u001B[0;32m    478\u001B[0m trees \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m--> 479\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_estimator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mappend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_more_estimators)\n\u001B[0;32m    481\u001B[0m ]\n\u001B[0;32m    483\u001B[0m \u001B[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001B[39;00m\n\u001B[0;32m    484\u001B[0m \u001B[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001B[39;00m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;66;03m# making threading more efficient than multiprocessing in\u001B[39;00m\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001B[39;00m\n\u001B[0;32m    487\u001B[0m \u001B[38;5;66;03m# parallel_backend contexts set at a higher level,\u001B[39;00m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;66;03m# since correctness does not rely on using threads.\u001B[39;00m\n\u001B[0;32m    489\u001B[0m trees \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[0;32m    490\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_jobs,\n\u001B[0;32m    491\u001B[0m     verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(trees)\n\u001B[0;32m    508\u001B[0m )\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:142\u001B[0m, in \u001B[0;36mBaseEnsemble._make_estimator\u001B[1;34m(self, append, random_state)\u001B[0m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Make and configure a copy of the `estimator_` attribute.\u001B[39;00m\n\u001B[0;32m    137\u001B[0m \n\u001B[0;32m    138\u001B[0m \u001B[38;5;124;03mWarning: This method should be used to properly instantiate new\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;124;03msub-estimators.\u001B[39;00m\n\u001B[0;32m    140\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    141\u001B[0m estimator \u001B[38;5;241m=\u001B[39m clone(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator_)\n\u001B[1;32m--> 142\u001B[0m \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_params\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43mp\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m random_state \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    145\u001B[0m     _set_random_states(estimator, random_state)\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:272\u001B[0m, in \u001B[0;36mBaseEstimator.set_params\u001B[1;34m(self, **params)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m params:\n\u001B[0;32m    270\u001B[0m     \u001B[38;5;66;03m# Simple optimization to gain speed (inspect is slow)\u001B[39;00m\n\u001B[0;32m    271\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n\u001B[1;32m--> 272\u001B[0m valid_params \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_params\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    274\u001B[0m nested_params \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mdict\u001B[39m)  \u001B[38;5;66;03m# grouped by prefix\u001B[39;00m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m params\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:243\u001B[0m, in \u001B[0;36mBaseEstimator.get_params\u001B[1;34m(self, deep)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    229\u001B[0m \u001B[38;5;124;03mGet parameters for this estimator.\u001B[39;00m\n\u001B[0;32m    230\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    240\u001B[0m \u001B[38;5;124;03m    Parameter names mapped to their values.\u001B[39;00m\n\u001B[0;32m    241\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    242\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m--> 243\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_param_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    244\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, key)\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m deep \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(value, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_params\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, \u001B[38;5;28mtype\u001B[39m):\n",
      "File \u001B[1;32mF:\\PyCharmProjects\\DataMining\\.venv\\Lib\\site-packages\\sklearn\\base.py:208\u001B[0m, in \u001B[0;36mBaseEstimator._get_param_names\u001B[1;34m(cls)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[0;32m    206\u001B[0m \u001B[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001B[39;00m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;66;03m# to represent\u001B[39;00m\n\u001B[1;32m--> 208\u001B[0m init_signature \u001B[38;5;241m=\u001B[39m \u001B[43minspect\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[43m(\u001B[49m\u001B[43minit\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001B[39;00m\n\u001B[0;32m    210\u001B[0m parameters \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    211\u001B[0m     p\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m init_signature\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mvalues()\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m p\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m p\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m!=\u001B[39m p\u001B[38;5;241m.\u001B[39mVAR_KEYWORD\n\u001B[0;32m    214\u001B[0m ]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3280\u001B[0m, in \u001B[0;36msignature\u001B[1;34m(obj, follow_wrapped, globals, locals, eval_str)\u001B[0m\n\u001B[0;32m   3278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msignature\u001B[39m(obj, \u001B[38;5;241m*\u001B[39m, follow_wrapped\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28mglobals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, eval_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   3279\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 3280\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_wrapped\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_wrapped\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3281\u001B[0m \u001B[43m                                   \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_str\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:3028\u001B[0m, in \u001B[0;36mSignature.from_callable\u001B[1;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001B[0m\n\u001B[0;32m   3024\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m   3025\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_callable\u001B[39m(\u001B[38;5;28mcls\u001B[39m, obj, \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   3026\u001B[0m                   follow_wrapped\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28mglobals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, eval_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   3027\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001B[39;00m\n\u001B[1;32m-> 3028\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_signature_from_callable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msigcls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3029\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mfollow_wrapper_chains\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_wrapped\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3030\u001B[0m \u001B[43m                                    \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_str\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:2516\u001B[0m, in \u001B[0;36m_signature_from_callable\u001B[1;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001B[0m\n\u001B[0;32m   2511\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m sig\u001B[38;5;241m.\u001B[39mreplace(parameters\u001B[38;5;241m=\u001B[39mnew_params)\n\u001B[0;32m   2513\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m isfunction(obj) \u001B[38;5;129;01mor\u001B[39;00m _signature_is_functionlike(obj):\n\u001B[0;32m   2514\u001B[0m     \u001B[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001B[39;00m\n\u001B[0;32m   2515\u001B[0m     \u001B[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001B[39;00m\n\u001B[1;32m-> 2516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_signature_from_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msigcls\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2517\u001B[0m \u001B[43m                                    \u001B[49m\u001B[43mskip_bound_arg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_bound_arg\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2518\u001B[0m \u001B[43m                                    \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_str\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_str\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2520\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _signature_is_builtin(obj):\n\u001B[0;32m   2521\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _signature_from_builtin(sigcls, obj,\n\u001B[0;32m   2522\u001B[0m                                    skip_bound_arg\u001B[38;5;241m=\u001B[39mskip_bound_arg)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\inspect.py:2333\u001B[0m, in \u001B[0;36m_signature_from_function\u001B[1;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001B[0m\n\u001B[0;32m   2328\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno signature found for builtin \u001B[39m\u001B[38;5;132;01m{!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(func))\n\u001B[0;32m   2330\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _signature_fromstr(\u001B[38;5;28mcls\u001B[39m, func, s, skip_bound_arg)\n\u001B[1;32m-> 2333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_signature_from_function\u001B[39m(\u001B[38;5;28mcls\u001B[39m, func, skip_bound_arg\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   2334\u001B[0m                              \u001B[38;5;28mglobals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mlocals\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, eval_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   2335\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Private helper: constructs Signature for the given python function.\"\"\"\u001B[39;00m\n\u001B[0;32m   2337\u001B[0m     is_duck_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# # Forward feature selection with hyperparameter tuning\n",
    "# best_subset, best_params, best_score = forward_feat_selection_hypertuning(RandomForestClassifier(), param_grid, X_encoded, X_train, y_train, X_test, y_test)\n",
    "# \n",
    "# print(\"Best subset of features:\", best_subset)\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Best model accuracy:\", best_score)\n",
    "# \n",
    "# # Use the best subset and best hyperparameters for final model\n",
    "# final_model = RandomForestClassifier(**best_params)\n",
    "# final_model.fit(X_train[best_subset], y_train)\n",
    "# final_model_preds = final_model.predict(X_test[best_subset])\n",
    "# final_model_accuracy = accuracy_score(y_test, final_model_preds)\n",
    "# \n",
    "# print(classification_report(y_test, final_model_preds))\n",
    "# \n",
    "# print_male_female_metrics(final_model, X, X_male, X_female, X_test[best_subset], y_test)\n",
    "# save_model(final_model, '../output/saved_models/rf_model_sexincl_ffs_tuned_unfair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T03:38:41.465231700Z",
     "start_time": "2024-05-05T02:53:54.218701900Z"
    }
   },
   "id": "c842c9cbe877c308",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3644d9483e4a363"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Forward feature selection with hyperparameter tuning\n",
    "# best_subset, best_params, best_score = forward_feat_selection_hypertuning(RandomForestClassifier(), param_grid, X_encoded, X_train, y_train, X_test, y_test, ensure_fairness=True)\n",
    "# \n",
    "# print(\"Best subset of features:\", best_subset)\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Best model accuracy:\", best_score)\n",
    "# \n",
    "# # Use the best subset and best hyperparameters for final model\n",
    "# final_model = RandomForestClassifier(**best_params)\n",
    "# final_model.fit(X_train[best_subset], y_train)\n",
    "# final_model_preds = final_model.predict(X_test[best_subset])\n",
    "# final_model_accuracy = accuracy_score(y_test, final_model_preds)\n",
    "# \n",
    "# print(classification_report(y_test, final_model_preds))\n",
    "# \n",
    "# print_male_female_metrics(final_model, X, X_male, X_female, X_test[best_subset], y_test)\n",
    "# save_model(final_model, '../output/saved_models/rf_model_sexincl_ffs_tuned_fair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-05T03:38:41.460245Z"
    }
   },
   "id": "d76a4b780ce8ab56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Removing sex-related columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31447eadcf57cc2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we do Forward feature selection with hyperparameter tuning, but we exclude all sex-related columns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c6c8e22b3978c0a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Forward feature selection with hyperparameter tuning\n",
    "# columns_to_exclude = ['sex', 'gave birth this year', 'marital status']\n",
    "# X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "# \n",
    "# best_subset, best_params, best_score = forward_feat_selection_hypertuning(RandomForestClassifier(), param_grid, X_encoded, X_train_, y_train_, X_test_, y_test_)\n",
    "# \n",
    "# print(\"Best subset of features:\", best_subset)\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Best model accuracy:\", best_score)\n",
    "# \n",
    "# # Use the best subset and best hyperparameters for final model\n",
    "# final_model_sexexcl_ffs = RandomForestClassifier(**best_params)\n",
    "# final_model_sexexcl_ffs.fit(X_train_[best_subset], y_train_)\n",
    "# final_model_preds = final_model_sexexcl_ffs.predict(X_test_[best_subset])\n",
    "# final_model_accuracy = accuracy_score(y_test_, final_model_preds)\n",
    "# \n",
    "# print(classification_report(y_test_, final_model_preds))\n",
    "# \n",
    "# print_male_female_metrics(final_model_sexexcl_ffs, X, X_male, X_female, X_test_[best_subset], y_test_)\n",
    "# save_model(final_model_sexexcl_ffs, '../output/saved_models/rf_model_sexexcl_ffs_tuned_unfair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-05T03:38:41.463238Z"
    }
   },
   "id": "654b7975ea6ab667"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30daa5a9496bb34"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # Forward feature selection with hyperparameter tuning\n",
    "# columns_to_exclude = ['sex', 'gave birth this year', 'marital status']\n",
    "# X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "# \n",
    "# best_subset, best_params, best_score = forward_feat_selection_hypertuning(RandomForestClassifier(), param_grid, X_encoded, X_train_, y_train_, X_test_, y_test_, ensure_fairness=True)\n",
    "# \n",
    "# print(\"Best subset of features:\", best_subset)\n",
    "# print(\"Best hyperparameters:\", best_params)\n",
    "# print(\"Best model accuracy:\", best_score)\n",
    "# \n",
    "# # Use the best subset and best hyperparameters for final model\n",
    "# final_model_sexexcl_ffs = RandomForestClassifier(**best_params)\n",
    "# final_model_sexexcl_ffs.fit(X_train_[best_subset], y_train_)\n",
    "# final_model_preds = final_model_sexexcl_ffs.predict(X_test_[best_subset])\n",
    "# final_model_accuracy = accuracy_score(y_test_, final_model_preds)\n",
    "# \n",
    "# print(classification_report(y_test_, final_model_preds))\n",
    "# \n",
    "# print_male_female_metrics(final_model_sexexcl_ffs, X, X_male, X_female, X_test_[best_subset], y_test_)\n",
    "# save_model(final_model_sexexcl_ffs, '../output/saved_models/rf_model_sexexcl_ffs_tuned_fair.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T03:38:41.470218800Z",
     "start_time": "2024-05-05T03:38:41.467227200Z"
    }
   },
   "id": "81fc819e0f88e067"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### AdaBoost Classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1740d0b32f05cb6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Define the parameter grid for AdaBoost\n",
    "param_grid = {\n",
    "    'algorithm': ['SAMME', 'SAMME.R'],\n",
    "    'n_estimators': [50, 75, 100, 150],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'estimator': [RandomForestClassifier()],\n",
    "    'random_state': [42]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T03:38:49.179318200Z",
     "start_time": "2024-05-05T03:38:49.090865700Z"
    }
   },
   "id": "830c04e2c37c90f3",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we tune the hyperparameters of the AdaBoost model using the defined parameter grid and using all features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b89b126bf805e807"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model, best_accuracy = tune_model(AdaBoostClassifier(), X_encoded, X_train, y_train, X_test, y_test, param_grid)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "preds = best_model.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test, y_test)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_unfair_ada.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-05-05T03:38:49.175115500Z"
    }
   },
   "id": "56c2740bed70bf7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75ab84e7f0dc894a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "best_params, best_model, best_accuracy = tune_model(AdaBoostClassifier(), X_encoded, X_train, y_train, X_test, y_test, param_grid)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "preds = best_model.predict(X_test)\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test, y_test)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexincl_noffs_tuned_fair_ada.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4832cff47380323d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we tune the hyperparameters of the AdaBoost model using the defined parameter grid and using a **subset of features**. We exclude the sex-related columns."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3e16d65d4e9f99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_exclude = ['sex', 'gave birth this year', 'marital status']\n",
    "X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "\n",
    "best_params, best_model, best_accuracy = tune_model(AdaBoostClassifier(), X_encoded, X_train_, y_train_, X_test_, y_test_, param_grid)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "preds = best_model.predict(X_test_)\n",
    "print(classification_report(y_test_, preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test_, y_test_)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexexcl_noffs_tuned_unfair_ada.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3ca3a4d9929f2568"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the same but with fairness constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd900b38ced9b26"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "columns_to_exclude = ['sex', 'gave birth this year', 'marital status']\n",
    "X_train_, X_test_, y_train_, y_test_, X_male, X_female = get_train_test_with_excluded_columns(data, columns_to_exclude)\n",
    "\n",
    "best_params, best_model, best_accuracy = tune_model(AdaBoostClassifier(), X_encoded, X_train_, y_train_, X_test_, y_test_, param_grid, ensure_fairness=True)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Model:\", best_model)\n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "\n",
    "preds = best_model.predict(X_test_)\n",
    "print(classification_report(y_test_, preds))\n",
    "\n",
    "print_male_female_metrics(best_model, X, X_male, X_female, X_test_, y_test_)\n",
    "save_model(best_model, '../output/saved_models/rf_model_sexexcl_noffs_tuned_fair_ada.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3ffb2b72c1d2287c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
